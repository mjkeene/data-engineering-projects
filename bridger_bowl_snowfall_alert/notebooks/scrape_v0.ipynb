{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f87680-5081-4400-affd-3c1684d8d4b6",
   "metadata": {},
   "source": [
    "<h3>Bridger Bowl Snowfall Alert Notification Project</h3>\n",
    "\n",
    "I'd like to receive a text every morning showing the snowfall details at Bridger Bowl from their website: https://bridgerbowl.com/weather/snow-report.\n",
    "\n",
    "This notebook is to get the backend logic sorted out that will eventually end up on Lambda and scheduled to run each day to get an update. This is a basic version for now that does not include retries, exponential backoff, logging, or any logic to check when that the last website update was after the last notification, etc. -- it's just a basic rough draft to get it up and running.\n",
    "\n",
    "I decided to use Selenium rather than BS4, since the snowfall data doesn't immediately appear on the website -- I can see it coming from some weather vendor service in the Network tab of Chrome page inspector. It's fairly slow to load (~1-3 seconds) when the page hasn't cached the data from a previous call, which is why BS4 wasn't able to load that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d004670-61bf-458b-ac5a-e0f847208fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_snowfall_data():\n",
    "    url = \"https://bridgerbowl.com/weather/snow-report\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    # print(response.text)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Modify the selector to match the site's structure\n",
    "    new_snow = soup.find(\"div\", class_=\"inline-flex flex-row text-2xl lg:text-3xl\") # new snow\n",
    "    print(new_snow)\n",
    "    \n",
    "    return new_snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916d9b4e-4202-489a-858b-d4ee60e15872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "get_snowfall_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fda70c-3c9a-4f8c-89ea-cac4791c94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_snowfall_data():\n",
    "    url = \"https://bridgerbowl.com/weather/snow-report\" \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the parent div with the class text-center\n",
    "    snow_depth_section = soup.find(\"div\", class_=\"text-center\")\n",
    "    if snow_depth_section:\n",
    "        # Find the specific div containing the snowfall value\n",
    "        base_snow_depth_element = snow_depth_section.find(\"div\", class_=\"inline-flex flex-row text-2xl lg:text-3xl\")\n",
    "        if base_snow_depth_element:\n",
    "            base_depth_value = snowfall_element.find(\"div\").text.strip()\n",
    "            return base_depth_value\n",
    "        else:\n",
    "            raise ValueError(\"Base snowfall value not found in the specified section\")\n",
    "    else:\n",
    "        raise ValueError(\"Base depth section not found on the webpage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abced149-cf83-4832-acea-43d032695d91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Base snowfall value not found in the specified section",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ba60261303a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_snowfall_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-51b893522bd8>\u001b[0m in \u001b[0;36mget_snowfall_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbase_depth_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Base snowfall value not found in the specified section\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Base depth section not found on the webpage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Base snowfall value not found in the specified section"
     ]
    }
   ],
   "source": [
    "get_snowfall_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea832eb-d4d1-43be-af5d-f1073b9272c5",
   "metadata": {},
   "source": [
    "<h3>Selenium over BS4</h3>\n",
    "\n",
    "BS4 is better than Selenium if the content being scraped is static (already available in raw HTML when the page loads). However, the content on Bridger Bowl's website is dynamically generated via JavaScript after the page initially loads, so BS4 cannot access it because it only parses the raw HTML of the response from the server.\n",
    "\n",
    "So, in this case, Selenium is the better option. The weather/snowfall data is not immediately loaded when the webpage is accessed. Selenium renders the page like a real browser, executing the JavaScript and exposing the final, updated DOM of the webpage (with the desired datapoints present).\n",
    "\n",
    "Note that, in general, Selenium is better suited for complex interactions -- i.e., scenarios where you need to interact with the page (e.g., clicking buttons, scrolling, or waiting for elements to load). I am using `WebDriverWait` to ensure the specific snowfall data I am targeting appears on the page before scraping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7beb72f-bcb6-41e3-94f6-90bfb844af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bridger Bowl Snow Report\n",
      "\n",
      "Last Updated: Friday, January 10th, 2025 at 4:00pm\n",
      "\n",
      "New Snow: 0 inches\n",
      "Recent Snow: 0 inches\n",
      "Base Depth: 52 inches\n",
      "Seasonal Snowfall: 120 inches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def get_snowfall_data_with_selenium():\n",
    "    \n",
    "    url = \"https://bridgerbowl.com/weather/snow-report\"\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Sections to extract\n",
    "        sections = [\"Base Depth\", \"New Snow\", \"Recent Snow\", \"Seasonal Snowfall\"]\n",
    "        snowfall_data = {}\n",
    "\n",
    "        wait = WebDriverWait(driver, 10)  # 10 seconds timeout\n",
    "\n",
    "        for section in sections:\n",
    "            try:\n",
    "                # Wait for each section to appear\n",
    "                section_element = wait.until(\n",
    "                    EC.presence_of_element_located((By.XPATH, f\"//div[h4[text()='{section}']]\"))\n",
    "                )\n",
    "\n",
    "                # Locate the specific nested div and extract the value\n",
    "                value_element = section_element.find_element(\n",
    "                    By.CSS_SELECTOR, \".inline-flex.flex-row.text-2xl.lg\\\\:text-3xl div\"\n",
    "                )\n",
    "                value = value_element.text.strip()\n",
    "                snowfall_data[section] = str(value) + \" inches\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting {section}: {e}\")\n",
    "                snowfall_data[section] = None  # Handle missing sections gracefully\n",
    "\n",
    "            # Extract the \"Last Updated\" section\n",
    "            try:\n",
    "                last_updated_element = wait.until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \".text-sm.sm\\\\:text-md.md\\\\:text-base\"))\n",
    "                )\n",
    "                last_updated_text = last_updated_element.text.strip()\n",
    "                last_updated_text = last_updated_text.replace(\"Last Updated \", '')\n",
    "                snowfall_data[\"Last Updated\"] = last_updated_text\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting Last Updated: {e}\")\n",
    "                snowfall_data[\"Last Updated\"] = None\n",
    "\n",
    "        return snowfall_data\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run the scraper\n",
    "snowfall = get_snowfall_data_with_selenium()\n",
    "\n",
    "# Format my message that will be sent out via SMS notification from AWS\n",
    "message = (\n",
    "    f\"Bridger Bowl Snow Report\\n\\n\"\n",
    "    f\"Last Updated: {snowfall['Last Updated']}\\n\\n\"\n",
    "    f\"New Snow: {snowfall['New Snow']}\\n\"\n",
    "    f\"Recent Snow: {snowfall['Recent Snow']}\\n\"\n",
    "    f\"Base Depth: {snowfall['Base Depth']}\\n\"\n",
    "    f\"Seasonal Snowfall: {snowfall['Seasonal Snowfall']}\\n\"\n",
    "\n",
    ")\n",
    "\n",
    "# View the final message that will be sent out to subscribers\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773baf57-b0f7-443d-aa7b-e7730723859b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68416ed3-d638-4c71-8596-6ac01816a2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
